{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41b9983a-a6de-4b27-b8e4-fcfc6935ccaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001584 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2134\n",
      "[LightGBM] [Info] Number of data points in the train set: 6818, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 7.309469\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000967 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2134\n",
      "[LightGBM] [Info] Number of data points in the train set: 6818, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 7.304180\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001119 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2137\n",
      "[LightGBM] [Info] Number of data points in the train set: 6818, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 7.301239\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001036 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2138\n",
      "[LightGBM] [Info] Number of data points in the train set: 6819, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 7.286993\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001254 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2139\n",
      "[LightGBM] [Info] Number of data points in the train set: 6819, number of used features: 53\n",
      "[LightGBM] [Info] Start training from score 7.287439\n",
      "✅ submission.csv created successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# 1. Load Data\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "y = np.log1p(train[\"Item_Outlet_Sales\"])\n",
    "train.drop(\"Item_Outlet_Sales\", axis=1, inplace=True)\n",
    "\n",
    "# 2. Create Item_Type_Combined early\n",
    "for df in [train, test]:\n",
    "    df[\"Item_Type_Combined\"] = df[\"Item_Identifier\"].str[:2].map({\n",
    "        \"FD\": \"Food\", \"DR\": \"Drinks\", \"NC\": \"Non-Consumable\"\n",
    "    })\n",
    "\n",
    "# 3. Combine for preprocessing\n",
    "data = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "\n",
    "# 4. Cleaning\n",
    "data[\"Item_Fat_Content\"] = data[\"Item_Fat_Content\"].replace({\n",
    "    \"LF\": \"Low Fat\", \"low fat\": \"Low Fat\", \"reg\": \"Regular\"\n",
    "})\n",
    "data[\"Item_Weight\"] = data.groupby(\"Item_Identifier\")[\"Item_Weight\"] \\\n",
    "    .transform(lambda x: x.fillna(x.mean()))\n",
    "data[\"Item_Weight\"].fillna(data[\"Item_Weight\"].mean(), inplace=True)\n",
    "\n",
    "outlet_size_mode = data.groupby(\"Outlet_Type\")[\"Outlet_Size\"].agg(lambda x: x.mode().iloc[0])\n",
    "data[\"Outlet_Size\"] = data.apply(lambda row: outlet_size_mode[row[\"Outlet_Type\"]] \n",
    "                                 if pd.isna(row[\"Outlet_Size\"]) else row[\"Outlet_Size\"], axis=1)\n",
    "\n",
    "data[\"Item_Visibility\"] = data[\"Item_Visibility\"].replace(0, np.nan)\n",
    "data[\"Item_Visibility\"] = data.groupby(\"Item_Identifier\")[\"Item_Visibility\"] \\\n",
    "    .transform(lambda x: x.fillna(x.mean()))\n",
    "data[\"Item_Visibility\"].fillna(data[\"Item_Visibility\"].mean(), inplace=True)\n",
    "\n",
    "# 5. Feature Engineering\n",
    "data[\"Outlet_Age\"] = 2025 - data[\"Outlet_Establishment_Year\"]\n",
    "data[\"Item_MRP_Tier\"] = pd.cut(data[\"Item_MRP\"], bins=[0,69,136,203,270],\n",
    "                               labels=[\"Low\",\"Medium\",\"High\",\"Very High\"])\n",
    "data[\"Price_per_Weight\"] = data[\"Item_MRP\"] / (data[\"Item_Weight\"] + 1)\n",
    "data[\"Visibility_log\"] = np.log1p(data[\"Item_Visibility\"])\n",
    "data[\"MRP_log\"] = np.log1p(data[\"Item_MRP\"])\n",
    "data[\"MRP_by_Age\"] = data[\"Item_MRP\"] / (data[\"Outlet_Age\"] + 1)\n",
    "\n",
    "# Target encoding features\n",
    "train_target = pd.DataFrame({\n",
    "    \"Item_Identifier\": train[\"Item_Identifier\"],\n",
    "    \"Outlet_Identifier\": train[\"Outlet_Identifier\"],\n",
    "    \"Item_Type_Combined\": train[\"Item_Type_Combined\"],\n",
    "    \"y\": np.expm1(y)\n",
    "})\n",
    "item_mean = train_target.groupby(\"Item_Identifier\")[\"y\"].mean().rename(\"Mean_Item_Sales\")\n",
    "outlet_mean = train_target.groupby(\"Outlet_Identifier\")[\"y\"].mean().rename(\"Mean_Outlet_Sales\")\n",
    "type_mean = train_target.groupby(\"Item_Type_Combined\")[\"y\"].mean().rename(\"Mean_Type_Sales\")\n",
    "\n",
    "data = data.merge(item_mean, on=\"Item_Identifier\", how=\"left\")\n",
    "data = data.merge(outlet_mean, on=\"Outlet_Identifier\", how=\"left\")\n",
    "data = data.merge(type_mean, on=\"Item_Type_Combined\", how=\"left\")\n",
    "\n",
    "for col in [\"Mean_Item_Sales\", \"Mean_Outlet_Sales\", \"Mean_Type_Sales\"]:\n",
    "    data[col].fillna(data[col].mean(), inplace=True)\n",
    "\n",
    "# 6. Split Back\n",
    "train_clean = data.iloc[:len(y), :]\n",
    "test_clean = data.iloc[len(y):, :]\n",
    "\n",
    "# Ensure correct dtypes for preprocessing\n",
    "categorical_cols = train_clean.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "numeric_cols = train_clean.select_dtypes(exclude=[\"object\"]).columns.tolist()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    train_clean[col] = train_clean[col].fillna(\"missing\").astype(str)\n",
    "    test_clean[col] = test_clean[col].fillna(\"missing\").astype(str)\n",
    "\n",
    "for col in numeric_cols:\n",
    "    train_clean[col] = pd.to_numeric(train_clean[col], errors=\"coerce\")\n",
    "    test_clean[col] = pd.to_numeric(test_clean[col], errors=\"coerce\")\n",
    "\n",
    "# 7. Preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", \"passthrough\", numeric_cols),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), categorical_cols)\n",
    "])\n",
    "\n",
    "X_all_enc = preprocessor.fit_transform(train_clean)\n",
    "X_test_enc = preprocessor.transform(test_clean)\n",
    "\n",
    "# 8. Model parameters\n",
    "xgb_params = dict(\n",
    "    n_estimators=800, learning_rate=0.05, max_depth=8,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    random_state=RANDOM_STATE, n_jobs=-1, tree_method=\"hist\",\n",
    "    eval_metric=\"rmse\"\n",
    ")\n",
    "lgb_params = dict(\n",
    "    n_estimators=800, learning_rate=0.05, max_depth=-1,\n",
    "    subsample=0.8, colsample_bytree=0.8, random_state=RANDOM_STATE\n",
    ")\n",
    "cat_params = dict(\n",
    "    iterations=800, learning_rate=0.05, depth=8,\n",
    "    silent=True, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# 9. 5-Fold OOF Stacking\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "ridge_oof = np.zeros((len(train_clean), 3))\n",
    "ridge_test_folds = np.zeros((len(test_clean), 3, 5))\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(kf.split(X_all_enc)):\n",
    "    X_tr, X_va = X_all_enc[tr_idx], X_all_enc[va_idx]\n",
    "    y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "    \n",
    "    xgb_model = XGBRegressor(**xgb_params)\n",
    "    xgb_model.fit(X_tr, y_tr)\n",
    "    \n",
    "    lgb_model = LGBMRegressor(**lgb_params)\n",
    "    lgb_model.fit(X_tr.toarray(), y_tr)  # LGBM needs dense\n",
    "    \n",
    "    cat_model = CatBoostRegressor(**cat_params)\n",
    "    cat_model.fit(X_tr.toarray(), y_tr, verbose=False)  # CatBoost needs dense\n",
    "    \n",
    "    ridge_oof[va_idx, 0] = xgb_model.predict(X_va)\n",
    "    ridge_oof[va_idx, 1] = lgb_model.predict(X_va.toarray())\n",
    "    ridge_oof[va_idx, 2] = cat_model.predict(X_va.toarray())\n",
    "    \n",
    "    ridge_test_folds[:, 0, fold] = xgb_model.predict(X_test_enc)\n",
    "    ridge_test_folds[:, 1, fold] = lgb_model.predict(X_test_enc.toarray())\n",
    "    ridge_test_folds[:, 2, fold] = cat_model.predict(X_test_enc.toarray())\n",
    "\n",
    "ridge_test_mean = ridge_test_folds.mean(axis=2)\n",
    "\n",
    "# 10. Ridge Meta learner\n",
    "meta_model = Ridge(alpha=1.0)\n",
    "meta_model.fit(ridge_oof, y)\n",
    "final_preds_log = meta_model.predict(ridge_test_mean)\n",
    "final_preds = np.expm1(final_preds_log)\n",
    "\n",
    "# 11. Save Submission\n",
    "submission = pd.DataFrame({\n",
    "    \"Item_Identifier\": test[\"Item_Identifier\"],\n",
    "    \"Outlet_Identifier\": test[\"Outlet_Identifier\"],\n",
    "    \"Item_Outlet_Sales\": final_preds\n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"✅ submission.csv created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b7ded2-0c3a-4410-b6bc-60f4cf1e0a54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
