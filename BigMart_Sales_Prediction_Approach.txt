The goal was to develop a high-performing, stable sales prediction pipeline for the Big Mart dataset. I began by exploring the data structure, identifying missing values, distinguishing between categorical and numeric features, and spotting opportunities for feature engineering. Initial experiments surfaced issues such as dtype=object in sparse matrices and breaking changes in scikit-learn and boosting libraries. To address these, I standardized preprocessing by filling all categorical features with defaults, casting them to strings, and coercing numeric features to proper numeric types.

The first working pipeline used a ColumnTransformer with OneHotEncoder for categorical encoding, directly incorporating numeric features, and engineering advanced variables such as Price_per_Weight, log-transformed values, and target-encoded means for items and outlets. While I initially applied Optuna to tune XGBoost, LightGBM, and CatBoost within a stacking framework, this proved computationally expensive, so I switched to strong fixed hyperparameters identified from earlier tuning runs.

The modeling strategy used 5-fold Out-of-Fold stacking: XGBoost, LightGBM, and CatBoost served as base learners, generating OOF predictions that fed into a Ridge regression meta-model. I ensured full compatibility with current library APIs by removing deprecated parameters (e.g., sparse → sparse_output), restructuring early stopping for XGBoost only, and converting sparse matrices to dense for models that don’t support sparse input.

Through iterative debugging—resolving KeyErrors, type mismatches, and API compatibility issues—the final pipeline became fast, reliable, and leaderboard-ready, consistently producing valid submission.csv files while balancing accuracy and computational efficiency.